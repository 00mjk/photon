From 96aac75ec990389568450d709a01af51df7deade Mon Sep 17 00:00:00 2001
From: Ajay Kaher <akaher@vmware.com>
Date: Tue, 29 Sep 2020 12:24:10 +0530
Subject: [PATCH] fs, 9p: write pages together if pages are consecutive in page
 cache.

Current implementation of writepages does generic
page cache write using write_cache_pages() with
per-page v9fs_vfs_writepage_locked() which calls
to network p9_client_write() with only one page at
a time.

To improve the performance of writepages calling
network p9_client_write() with multiple pages request
at a time if pages are consecutive in page cache.

Signed-off-by: Ajay Kaher <akaher@vmware.com>
---
 fs/9p/vfs_addr.c | 298 +++++++++++++++++++++++++++++++++++++++++++++++++++++++
 1 file changed, 298 insertions(+)

diff --git a/fs/9p/vfs_addr.c b/fs/9p/vfs_addr.c
index 2bfb53a..2409ecd 100644
--- a/fs/9p/vfs_addr.c
+++ b/fs/9p/vfs_addr.c
@@ -460,12 +460,310 @@ static int v9fs_write_end(struct file *filp, struct address_space *mapping,
 	return copied;
 }
 
+static inline bool is_interrupt_error(int error)
+{
+	switch (error) {
+	case -EINTR:
+	case -ERESTARTSYS:
+	case -ERESTARTNOHAND:
+	case -ERESTARTNOINTR:
+		return true;
+	}
+	return false;
+}
+
+static inline bool v9fs_is_retryable_error(int error)
+{
+	if (is_interrupt_error(error) || error == -EAGAIN)
+		return true;
+	return false;
+}
+
+void v9fs_writev_complete(struct page **wpages, unsigned int nr_pages, int result)
+{
+	int i;
+
+	for (i = 0; i < nr_pages; i++) {
+		struct page *page = wpages[i];
+		if (result == -EAGAIN)
+			__set_page_dirty_nobuffers(page);
+		else if (result < 0)
+			SetPageError(page);
+
+		end_page_writeback(page);
+		put_page(page);
+	}
+
+	if (result != -EAGAIN)
+		mapping_set_error(wpages[0]->mapping, result);
+}
+
+static struct page ** v9fs_alloc_and_fillpages( pgoff_t tofind,
+		struct address_space *mapping, pgoff_t end, pgoff_t *index,
+		unsigned int *found_pages)
+{
+	struct page **wpages = kcalloc(tofind, sizeof(struct page *), GFP_NOFS);
+
+	if (!wpages)
+		return NULL;
+
+	*found_pages = find_get_pages_range_tag(mapping, index, end,
+				PAGECACHE_TAG_DIRTY, tofind, wpages);
+	return wpages;
+}
+
+static unsigned int v9fs_prepare_write_pages(struct page **wpages,
+		unsigned int found_pages, struct address_space *mapping,
+		struct writeback_control *wbc, pgoff_t end,
+		pgoff_t *index, pgoff_t *next, bool *done)
+{
+	unsigned int nr_pages = 0, i;
+	struct page *page;
+
+	for (i = 0; i < found_pages; i++) {
+		page = wpages[i];
+		/*
+		 * At this point we hold neither the i_pages lock nor the
+		 * page lock: the page may be truncated or invalidated
+		 * (changing page->mapping to NULL), or even swizzled
+		 * back from swapper_space to tmpfs file mapping
+		 */
+
+		if (nr_pages == 0)
+			lock_page(page);
+		else if (!trylock_page(page))
+			break;
+
+		if (unlikely(page->mapping != mapping)) {
+			unlock_page(page);
+			break;
+		}
+
+		if (!wbc->range_cyclic && page->index > end) {
+			*done = true;
+			unlock_page(page);
+			break;
+		}
+
+		if (*next && (page->index != *next)) {
+			/* Not next consecutive page */
+			unlock_page(page);
+			break;
+		}
+
+		if (wbc->sync_mode != WB_SYNC_NONE)
+			wait_on_page_writeback(page);
+
+		if (PageWriteback(page) ||
+				!clear_page_dirty_for_io(page)) {
+			unlock_page(page);
+			break;
+		}
+
+		/*
+		 * This actually clears the dirty bit in the radix tree.
+		 * See cifs_writepage() for more commentary.
+		 */
+		set_page_writeback(page);
+		if (page_offset(page) >= i_size_read(mapping->host)) {
+			*done = true;
+			unlock_page(page);
+			end_page_writeback(page);
+			break;
+		}
+
+		*next = page->index + 1;
+		++nr_pages;
+	}
+
+	/* reset index to refind any pages skipped */
+	if (nr_pages == 0)
+		*index = wpages[0]->index + 1;
+
+	/* put any pages we aren't going to use */
+	for (i = nr_pages; i < found_pages; i++) {
+		put_page(wpages[i]);
+		wpages[i] = NULL;
+	}
+	return nr_pages;
+}
+
+static int v9fs_send_write_pages(struct page **wpages,
+		unsigned int nr_pages, struct address_space *mapping,
+		struct writeback_control *wbc)
+{
+	int err = 0;
+	unsigned int i;
+	unsigned int bytes;
+	unsigned int tailsz;
+
+	struct iov_iter from;
+	struct bio_vec *bvec;
+
+	struct inode *inode = mapping->host;
+	struct v9fs_inode *v9inode = V9FS_I(inode);
+
+	tailsz = min(i_size_read(mapping->host) -
+			page_offset(wpages[nr_pages - 1]),
+			(loff_t)PAGE_SIZE);
+	bytes = ((nr_pages - 1) * PAGE_SIZE) + tailsz;
+
+	bvec = kcalloc(nr_pages, sizeof(struct bio_vec), GFP_KERNEL);
+	for (i = 0; i < nr_pages; i++) {
+		bvec[i].bv_page = wpages[i];
+		bvec[i].bv_len = PAGE_SIZE;
+	}
+
+	bvec[i-1].bv_len = tailsz; /* set the len of last page */
+	iov_iter_bvec(&from, ITER_BVEC | WRITE, bvec, nr_pages, bytes);
+	p9_client_write(v9inode->writeback_fid, page_offset(bvec->bv_page), &from, &err);
+
+	for (i = 0; i < nr_pages; ++i)
+		unlock_page(wpages[i]);
+
+	kfree(bvec);
+	return err;
+}
+
+static int v9fs_vfs_writepages(struct address_space *mapping,
+			   struct writeback_control *wbc)
+{
+
+	struct inode *inode = mapping->host;
+	struct v9fs_session_info *v9ses = v9fs_inode2v9ses(inode);
+	unsigned int msize = v9ses->clnt->msize;
+
+	bool done = false, scanned = false, range_whole = false;
+	pgoff_t end, index;
+	struct page **wpages = NULL;
+	int rc = 0;
+	int saved_rc = 0;
+
+	/*
+	 * If wsize is smaller than the page cache size, default to writing
+	 * one page at a time via cifs_writepage
+	 */
+	if (msize < PAGE_SIZE)
+		return generic_writepages(mapping, wbc);
+
+	if (wbc->range_cyclic) {
+		index = mapping->writeback_index; /* Start from prev offset */
+		end = -1;
+	} else {
+		index = wbc->range_start >> PAGE_SHIFT;
+		end = wbc->range_end >> PAGE_SHIFT;
+		if (wbc->range_start == 0 && wbc->range_end == LLONG_MAX)
+			range_whole = true;
+		scanned = true;
+	}
+retry:
+	while (!done && index <= end) {
+		unsigned int i, nr_pages, found_pages;
+		pgoff_t next = 0, tofind, saved_index = index;
+
+		tofind = min(((msize - P9_IOHDRSZ)/ PAGE_SIZE) - 1, end - index) + 1;
+		wpages = v9fs_alloc_and_fillpages(tofind, mapping, end, &index,
+						  &found_pages);
+		if (!wpages) {
+			rc = -ENOMEM;
+			done = true;
+			break;
+		}
+
+		if (found_pages == 0) {
+			if (wpages) {
+				kfree(wpages);
+				wpages = NULL;
+			}
+			break;
+		}
+
+		nr_pages = v9fs_prepare_write_pages(wpages, found_pages, mapping, wbc,
+					       end, &index, &next, &done);
+
+		/* nothing to write? */
+		if (nr_pages == 0) {
+			if (wpages) {
+				kfree(wpages);
+				wpages = NULL;
+			}
+			continue;
+		}
+
+		rc = v9fs_send_write_pages(wpages, nr_pages, mapping, wbc);
+
+		/* send failure -- clean up the mess */
+		if (rc != 0) {
+			for (i = 0; i < nr_pages; ++i) {
+				if (v9fs_is_retryable_error(rc))
+					redirty_page_for_writepage(wbc,
+							   wpages[i]);
+				else
+					SetPageError(wpages[i]);
+				end_page_writeback(wpages[i]);
+				put_page(wpages[i]);
+			}
+			if (!v9fs_is_retryable_error(rc))
+				mapping_set_error(mapping, rc);
+		} else {
+			v9fs_writev_complete(wpages, nr_pages, rc);
+		}
+
+		if (wpages) {
+			kfree(wpages);
+			wpages = NULL;
+		}
+		if (wbc->sync_mode == WB_SYNC_ALL && rc == -EAGAIN) {
+			index = saved_index;
+			continue;
+		}
+
+		/* Return immediately if we received a signal during writing */
+		if (is_interrupt_error(rc)) {
+			done = true;
+			break;
+		}
+
+		if (rc != 0 && saved_rc == 0)
+			saved_rc = rc;
+
+		wbc->nr_to_write -= nr_pages;
+		if (wbc->nr_to_write <= 0)
+			done = true;
+
+		index = next;
+
+		if (rc != 0) {
+			done = true;
+			break;
+		}
+	}
+
+	if (!scanned && !done) {
+		/*
+		 * We hit the last page and there is more work to be done: wrap
+		 * back to the start of the file
+		 */
+		scanned = true;
+		index = 0;
+		goto retry;
+	}
+
+	if (saved_rc != 0)
+		rc = saved_rc;
+
+	if (wbc->range_cyclic || (range_whole && wbc->nr_to_write > 0))
+		mapping->writeback_index = index;
+
+	return rc;
+}
 
 const struct address_space_operations v9fs_addr_operations = {
 	.readpage = v9fs_vfs_readpage,
 	.readpages = v9fs_vfs_readpages,
 	.set_page_dirty = __set_page_dirty_nobuffers,
 	.writepage = v9fs_vfs_writepage,
+	.writepages = v9fs_vfs_writepages,
 	.write_begin = v9fs_write_begin,
 	.write_end = v9fs_write_end,
 	.releasepage = v9fs_release_page,
-- 
2.7.4

